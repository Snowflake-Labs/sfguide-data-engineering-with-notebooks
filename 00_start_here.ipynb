{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2f7b3deb-2291-4369-93ff-7fc8efe3dd0f",
      "metadata": {
        "name": "md_intro",
        "title": "md_intro",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Snowflake Notebook Data Engineering\n\n* Author: Jeremiah Hansen\n* Last Updated: 2/12/2026\n\nWelcome to the beginning of the Quickstart! Please refer to [the official Snowflake Notebook Data Engineering Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/) for all the details including set up steps.",
      "execution_count": null
    },
    {
      "id": "57a2d0cd-156a-473b-8cb1-f7cd2d8e9a64",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 01 Setup Snowflake\n\nDuring this step we will create our demo environment."
    },
    {
      "id": "dbd8d35d-5da2-4855-99a1-b6df36a42b04",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_1",
        "language": "sql",
        "name": "sql_step01_setup_snowflake",
        "title": "sql_step01_setup_snowflake"
      },
      "source": "-- ----------------------------------------------------------------------------\n-- Create the account level objects (ACCOUNTADMIN part)\n-- ----------------------------------------------------------------------------\nSET MY_USER = CURRENT_USER();\nUSE ROLE ACCOUNTADMIN;\n\n-- Roles\nCREATE OR REPLACE ROLE DEMO_ROLE;\nGRANT ROLE DEMO_ROLE TO ROLE SYSADMIN;\nGRANT ROLE DEMO_ROLE TO USER IDENTIFIER($MY_USER);\n\nGRANT CREATE INTEGRATION ON ACCOUNT TO ROLE DEMO_ROLE;\nGRANT EXECUTE TASK ON ACCOUNT TO ROLE DEMO_ROLE;\nGRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE DEMO_ROLE;\nGRANT MONITOR EXECUTION ON ACCOUNT TO ROLE DEMO_ROLE;\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE DEMO_ROLE;\n\n-- Databases\nCREATE OR REPLACE DATABASE DEMO_DB;\nGRANT OWNERSHIP ON DATABASE DEMO_DB TO ROLE DEMO_ROLE;\n\n-- Warehouses\nCREATE OR REPLACE WAREHOUSE DEMO_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\nGRANT OWNERSHIP ON WAREHOUSE DEMO_WH TO ROLE DEMO_ROLE;\n\n\n-- ----------------------------------------------------------------------------\n-- Create the database level objects\n-- ----------------------------------------------------------------------------\nUSE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE DATABASE DEMO_DB;\n\n-- Schemas\nCREATE OR REPLACE SCHEMA INTEGRATIONS;\nCREATE OR REPLACE SCHEMA DEV_SCHEMA;\nCREATE OR REPLACE SCHEMA PROD_SCHEMA;\n\nUSE SCHEMA INTEGRATIONS;\n\n-- External Frostbyte objects\nCREATE OR REPLACE STAGE FROSTBYTE_RAW_STAGE\n    URL = 's3://sfquickstarts/data-engineering-with-snowpark-python/'\n;\n\n\n-- ----------------------------------------------------------------------------\n-- Create the event table\n-- ----------------------------------------------------------------------------\nUSE ROLE ACCOUNTADMIN;\n\nCREATE EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS;\nGRANT SELECT ON EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE DEMO_ROLE;\nGRANT INSERT ON EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE DEMO_ROLE;\n\nALTER ACCOUNT SET EVENT_TABLE = DEMO_DB.INTEGRATIONS.DEMO_EVENTS;\nALTER DATABASE DEMO_DB SET LOG_LEVEL = INFO;\n\n\n-- ----------------------------------------------------------------------------\n-- Set our new context\n-- ----------------------------------------------------------------------------\nUSE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE SCHEMA DEMO_DB.INTEGRATIONS;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "60106471-3d2f-4181-98c5-16edf63a4f47",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_9",
        "language": "sql",
        "name": "sql_step01_reset_connection",
        "title": "sql_step01_reset_connection"
      },
      "source": "%%sql -r dataframe_9\nUSE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE SCHEMA DEMO_DB.INTEGRATIONS;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a7aa7465-87d7-47ec-9ac8-7b82c41def64",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 02 Load Weather\n\nBut what about data that needs constant updating - like the WEATHER data? We would need to build a pipeline process to constantly update that data to keep it fresh.\n\nPerhaps a better way to get this external data would be to source it from a trusted data supplier. Let them manage the data, keeping it accurate and up to date.\n\nEnter the Snowflake Data Cloud...\n\nWeather Source is a leading provider of global weather and climate data and their OnPoint Product Suite provides businesses with the necessary weather and climate data to quickly generate meaningful and actionable insights for a wide range of use cases across industries. Let's connect to the \"Weather Source LLC: frostbyte\" feed from Weather Source in the Snowflake Data Marketplace by following these steps in Snowsight\n\n* In the left navigation bar click on \"Marketplace\" and then \"Snowflake Marketplace\"\n* Search: \"Weather Source LLC: frostbyte\" (and click on tile in results)\n* Click the blue \"Get\" button\n* Under \"Options\", adjust the Database name to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n* Grant to \"DEMO_ROLE\"\n\nThat's it... we don't have to do anything from here to keep this data updated. The provider will do that for us and data sharing means we are always seeing whatever they they have published."
    },
    {
      "id": "e54c68df-8cdb-401b-985c-cb9ca6b1ac4e",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_5",
        "language": "sql",
        "name": "sql_step02_create_share",
        "title": "sql_step02_create_share"
      },
      "source": "%%sql -r dataframe_5\n/*---\n-- You can also do it via code if you know the account/share details...\nSET WEATHERSOURCE_ACCT_NAME = '*** PUT ACCOUNT NAME HERE AS PART OF DEMO SETUP ***';\nSET WEATHERSOURCE_SHARE_NAME = '*** PUT ACCOUNT SHARE HERE AS PART OF DEMO SETUP ***';\nSET WEATHERSOURCE_SHARE = $WEATHERSOURCE_ACCT_NAME || '.' || $WEATHERSOURCE_SHARE_NAME;\n\nCREATE OR REPLACE DATABASE FROSTBYTE_WEATHERSOURCE\n  FROM SHARE IDENTIFIER($WEATHERSOURCE_SHARE);\n\nGRANT IMPORTED PRIVILEGES ON DATABASE FROSTBYTE_WEATHERSOURCE TO ROLE HOL_ROLE;\n---*/",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e2c0aa54-69d1-430b-b4e8-e9fef71b347d",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_6",
        "language": "sql",
        "name": "sql_step02_test_share",
        "title": "sql_step02_test_share"
      },
      "source": "%%sql -r dataframe_6\n-- Let's look at the data - same 3-part naming convention as any other table\nSELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "568ebe47-6c1c-46a8-9eee-28a15221e19a",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 03 Load Location and Order Detail\n\nPlease follow the instructions in the [Load Location and Order Details section of the Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/#load-location-and-order-detail) to open and run the `01_load_excel_files` Notebook. That Notebook will define the pipeline used to load data into the `LOCATION` and `ORDER_DETAIL` tables from the staged Excel files."
    },
    {
      "id": "8f76534f-8fba-4f7a-b03d-6cd5aad051e5",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 04 Load Daily City Metrics\n\nPlease follow the instructions in the [Load Daily City Metrics section of the Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/#load-daily-city-metrics) to open and run the `02_load_daily_city_metrics` Notebook. That Notebook will define the pipeline used to create the `DAILY_CITY_METRICS` table."
    },
    {
      "id": "70235fda-a0c8-4091-a985-768c50ea37f0",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "After running the two notebooks above we can now inspect the logs that were written to [the Snowflake Event table](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up). The query below will help get us started."
    },
    {
      "id": "33655819-e5a8-4eb5-8ab3-eedb250899fb",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_7",
        "language": "sql",
        "name": "sql_step04_logs",
        "title": "sql_step04_logs"
      },
      "source": "%%sql -r dataframe_7\nUSE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE SCHEMA DEMO_DB.INTEGRATIONS;\n\nSELECT \n    TIMESTAMP,\n    VALUE AS LOG_MESSAGE,\n    RESOURCE_ATTRIBUTES:\"snow.service.name\"::string AS SERVICE_NAME,\n    RECORD_ATTRIBUTES:\"severity_text\"::string AS SEVERITY\nFROM DEMO_DB.INTEGRATIONS.DEMO_EVENTS\nWHERE RECORD_TYPE = 'LOG'\n--  AND RESOURCE_ATTRIBUTES:\"snow.service.name\" = 'NOTEBOOK_SERVICE'\n  AND RESOURCE_ATTRIBUTES:\"snow.service.name\" NOT IN ('OPENFLOW', 'MYPOSTGRESCDC')\n  AND TIMESTAMP > DATEADD(hour, -1, CURRENT_TIMESTAMP())\nORDER BY TIMESTAMP DESC\nLIMIT 100;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ac3dea3c-9027-4899-85c0-3c3f6966518f",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 05 Deploy Dev Notebook Project\n\nDuring this step we will be deploying the dev versions of our two data engineering Notebooks: `01_load_excel_files` and `02_load_daily_city_metrics`. To deploy notebooks to Snowflake we will create a `NOTEBOOK PROJECT` object in our `DEV_SCHEMA`, along with the other resources in our development environment.\n\nNotebook Project Objects (NPOs) are the key to deploying and scheduling notebooks in production with Workspace Notebooks. An NPO is a schema-level object that encapsulates your notebook files and their dependencies, making them ready for scheduled execution.\n\nThis code is contained in the `scripts/deploy_notebooks.py` script which is used below and in the deployment CI/CD pipeline later. Please open and review the contents of the script."
    },
    {
      "id": "f1822582-3f5b-4deb-8f2f-edac9f8c3696",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "sql_step05_deploy_dev_notebooks",
        "title": "sql_step05_deploy_dev_notebooks"
      },
      "source": "%run scripts/deploy_notebooks.py DEMO_DB DEV_SCHEMA DEMO_PIPELINES_NP ./notebooks",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0b5c13db-2227-4583-a781-ca07a980418b",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 06 Orchestrate Pipelines\n\nIn this step we will create a DAG (or Directed Acyclic Graph) of Tasks using the new [Snowflake Python Management API](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-overview). The Task DAG API builds upon the Python Management API to provide advanced Task management capabilities. For more details see [Managing Snowflake tasks and task graphs with Python](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-managing-tasks).\n\nThis code is contained in the `scripts/deploy_task_dag.py` script which is used below and in the deployment CI/CD pipeline later. Please open and review the contents of the script.\n"
    },
    {
      "id": "7b24ff1b-46cd-4d26-83ad-60c437cb8bf9",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_step06_create_dag",
        "title": "py_step06_create_dag"
      },
      "source": "%run scripts/deploy_task_dag.py DEMO_DB DEV_SCHEMA DEMO_PIPELINES_NP",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "854cfd76-f228-480d-bc2b-e421ab521f91",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "In a new tab, switch to the Horizon Catalog, then open and review the new Task DAG that was created. Next manually execute the tasks by opening up the root `DEMO_DAG` task, clicking on the \"Graph\" tab and then clicking on the \"Run Task Graph\" play button above the diagram. If not already set, you may have to pick a warehouse to do this."
    },
    {
      "id": "1fa77bfd-0bb1-41c8-a759-575b4693ec70",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 07 Deploy to Production\n\nSteps\n1. Make a small change to a notebook and commit it to the dev branch\n1. Go into GitHub and create a PR and Merge to main branch\n1. Review GitHub Actions workflow definition and run results\n1. See new production versions of the Notebook Project object and Task DAG in the PROD_SCHEMA\n1. Run the production version of the task DAG and see new tables created!"
    },
    {
      "id": "56dd1678-8e30-4671-b4ea-828f668283d0",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 08 Teardown\n\nFinally, we will tear down our demo environment."
    },
    {
      "id": "77906fe6-875b-4518-b699-cb20bb9fe62f",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_8",
        "language": "sql",
        "name": "sql_step10",
        "title": "sql_step10"
      },
      "source": "%%sql -r dataframe_8\nUSE ROLE ACCOUNTADMIN;\n\nDROP DATABASE DEMO_DB;\nDROP WAREHOUSE DEMO_WH;\nDROP ROLE DEMO_ROLE;\n\n-- Drop the weather share\nDROP DATABASE FROSTBYTE_WEATHERSOURCE;\n\n-- Remove the \"dev\" branch in your repo",
      "outputs": [],
      "execution_count": null
    }
  ]
}