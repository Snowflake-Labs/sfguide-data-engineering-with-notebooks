{
  "metadata": {
    "kernelspec": {
      "name": "jupyter",
      "display_name": "Jupyter Notebook"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2f7b3deb-2291-4369-93ff-7fc8efe3dd0f",
      "metadata": {
        "name": "md_intro",
        "title": "md_intro",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Snowflake Notebook Data Engineering\n\n* Author: Jeremiah Hansen\n* Last Updated: 2/27/2026\n\nWelcome to the beginning of the Quickstart! Please refer to [the official Snowflake Notebook Data Engineering Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/) for all the details including set up steps.",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "57a2d0cd-156a-473b-8cb1-f7cd2d8e9a64",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 01 Setup Snowflake\n\nAt this point you should have already completed the instructions in the [Setup Snowflake section of the Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/#setup-snowflake) to create the Workspace, demo objects, and notebook service."
    },
    {
      "cell_type": "code",
      "id": "60106471-3d2f-4181-98c5-16edf63a4f47",
      "metadata": {
        "language": "sql",
        "name": "sql_step01_reset_connection",
        "title": "sql_step01_reset_connection",
        "resultVariableName": "dataframe_9"
      },
      "source": "%%sql -r dataframe_9\nUSE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE SCHEMA DEMO_DB.INTEGRATIONS;",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "a7aa7465-87d7-47ec-9ac8-7b82c41def64",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 02 Load Weather\n\nPlease follow the instructions in the [Load Weather section of the Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/#load-weather) to connect to the Weather Source data share in the Snowflake Marketplace. Here's a recap of the steps:\n\n* Login to Snowsight\n* Click on the `Marketplace` -> `Snowflake Marketplace` link in the left navigation bar\n* Enter \"Weather Source LLC: frostbyte\" in the search box and click return\n* Click on the \"Weather Source LLC: frostbyte\" listing tile\n* Click the blue \"Get\" button\n    * Expand the \"Options\" dialog\n    * Change the \"Database name\" to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n    * Select the \"DEMO_ROLE\" role to have access to the new database\n* Click on the blue \"Get\" button"
    },
    {
      "cell_type": "code",
      "id": "e54c68df-8cdb-401b-985c-cb9ca6b1ac4e",
      "metadata": {
        "language": "sql",
        "name": "sql_step02_create_share",
        "title": "sql_step02_create_share",
        "resultVariableName": "dataframe_5"
      },
      "source": "/*---\n-- You can also do it via code if you know the account/share details...\nSET WEATHERSOURCE_ACCT_NAME = '*** PUT ACCOUNT NAME HERE AS PART OF DEMO SETUP ***';\nSET WEATHERSOURCE_SHARE_NAME = '*** PUT ACCOUNT SHARE HERE AS PART OF DEMO SETUP ***';\nSET WEATHERSOURCE_SHARE = $WEATHERSOURCE_ACCT_NAME || '.' || $WEATHERSOURCE_SHARE_NAME;\n\nCREATE OR REPLACE DATABASE FROSTBYTE_WEATHERSOURCE\n  FROM SHARE IDENTIFIER($WEATHERSOURCE_SHARE);\n\nGRANT IMPORTED PRIVILEGES ON DATABASE FROSTBYTE_WEATHERSOURCE TO ROLE DEMO_ROLE;\n---*/",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "83148ab5-1879-4c3b-9bae-913151c12f64",
      "cell_type": "markdown",
      "metadata": {
        "name": "sql_ste",
        "title": "sql_ste",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "And here's a sample SQL query you can run to view some data from the Weather Source share:",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "e2c0aa54-69d1-430b-b4e8-e9fef71b347d",
      "metadata": {
        "language": "sql",
        "name": "sql_step02_test_share",
        "title": "sql_step02_test_share",
        "resultVariableName": "dataframe_6"
      },
      "source": "%%sql -r dataframe_6\n-- Let's look at the data - same 3-part naming convention as any other table\nSELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100;",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "568ebe47-6c1c-46a8-9eee-28a15221e19a",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 03 Load Excel Files\n\nPlease follow the instructions in the [Load Excel Files section of the Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/#load-excel-files) to open and run the `01_load_excel_files` Notebook. That Notebook will define the pipeline used to load data into the `LOCATION` and `ORDER_DETAIL` tables from the staged Excel files."
    },
    {
      "cell_type": "markdown",
      "id": "8f76534f-8fba-4f7a-b03d-6cd5aad051e5",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 04 Load Daily City Metrics\n\nPlease follow the instructions in the [Load Daily City Metrics section of the Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/#load-daily-city-metrics) to open and run the `02_load_daily_city_metrics` Notebook. That Notebook will define the pipeline used to create the `DAILY_CITY_METRICS` table."
    },
    {
      "cell_type": "markdown",
      "id": "70235fda-a0c8-4091-a985-768c50ea37f0",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "After running the two notebooks above we can now inspect the logs that were written to [the Snowflake Event table](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up). The query below will help get us started."
    },
    {
      "cell_type": "code",
      "id": "33655819-e5a8-4eb5-8ab3-eedb250899fb",
      "metadata": {
        "language": "sql",
        "name": "sql_step04_logs",
        "title": "sql_step04_logs",
        "resultVariableName": "dataframe_7"
      },
      "source": "USE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE SCHEMA DEMO_DB.INTEGRATIONS;\n\nSELECT \n    TIMESTAMP,\n    VALUE AS LOG_MESSAGE,\n    RESOURCE_ATTRIBUTES:\"snow.service.name\"::string AS SERVICE_NAME,\n    RECORD_ATTRIBUTES:\"severity_text\"::string AS SEVERITY\nFROM DEMO_DB.INTEGRATIONS.DEMO_EVENTS\nWHERE RECORD_TYPE = 'LOG'\n--  AND RESOURCE_ATTRIBUTES:\"snow.service.name\" = 'NOTEBOOK_SERVICE'\n  AND TIMESTAMP > DATEADD(hour, -1, CURRENT_TIMESTAMP())\nORDER BY TIMESTAMP DESC\nLIMIT 100;",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "ac3dea3c-9027-4899-85c0-3c3f6966518f",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 05 Deploy Dev Notebook Project\n\nDuring this step we will be deploying the dev versions of our two data engineering Notebooks: `01_load_excel_files` and `02_load_daily_city_metrics`. To deploy notebooks to Snowflake we will create a `NOTEBOOK PROJECT` object in our `DEV_SCHEMA`, along with the other resources in our development environment.\n\nNotebook Project Objects (NPOs) are the key to deploying and scheduling notebooks in production with Workspace Notebooks. An NPO is a schema-level object that encapsulates your notebook files and their dependencies, making them ready for scheduled execution.\n\nThis code is contained in the `scripts/deploy_notebooks.py` script which is used below and in the deployment CI/CD pipeline later. Please open and review the contents of the script."
    },
    {
      "cell_type": "code",
      "id": "f1822582-3f5b-4deb-8f2f-edac9f8c3696",
      "metadata": {
        "language": "python",
        "name": "sql_step05_deploy_dev_notebooks",
        "title": "sql_step05_deploy_dev_notebooks"
      },
      "source": "%run scripts/deploy_notebooks.py DEMO_DB DEV_SCHEMA DEMO_PIPELINES_NP ./notebooks",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "0b5c13db-2227-4583-a781-ca07a980418b",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 06 Orchestrate Pipelines\n\nIn this step we will create a DAG (or Directed Acyclic Graph) of Tasks using the new [Snowflake Python Management API](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-overview). The Task DAG API builds upon the Python Management API to provide advanced Task management capabilities. For more details see [Managing Snowflake tasks and task graphs with Python](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-managing-tasks).\n\nThis code is contained in the `scripts/deploy_task_dag.py` script which is used below and in the deployment CI/CD pipeline later. Please open and review the contents of the script.\n"
    },
    {
      "cell_type": "code",
      "id": "7b24ff1b-46cd-4d26-83ad-60c437cb8bf9",
      "metadata": {
        "language": "python",
        "name": "py_step06_create_dag",
        "title": "py_step06_create_dag"
      },
      "source": "%run scripts/deploy_task_dag.py DEMO_DB DEV_SCHEMA DEMO_PIPELINES_NP",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "854cfd76-f228-480d-bc2b-e421ab521f91",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "In a new tab, switch to the Horizon Catalog, then open and review the new Task DAG that was created. Next manually execute the tasks by opening up the root `DEMO_DAG` task, clicking on the \"Graph\" tab and then clicking on the \"Run Task Graph\" play button above the diagram. If not already set, you may have to pick a warehouse to do this."
    },
    {
      "cell_type": "markdown",
      "id": "1fa77bfd-0bb1-41c8-a759-575b4693ec70",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 07 Deploy to Production\n\nPlease follow the instructions in the [Deploy to Production section of the Quickstart](https://www.snowflake.com/en/developers/guides/data-engineering-with-notebooks/#deploy-to-production) to deploy our Notebooks and Task DAG to production.\n\nHere is an outline of the steps:\n1. Make a small change to a notebook and commit it to the dev branch\n1. Go into GitHub and create a PR and Merge to main branch\n1. Review GitHub Actions workflow definition and run results\n1. See new production versions of the Notebook Project object and Task DAG in the PROD_SCHEMA\n1. Run the production version of the task DAG and see new tables created!"
    },
    {
      "cell_type": "markdown",
      "id": "56dd1678-8e30-4671-b4ea-828f668283d0",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step 08 Teardown\n\nFinally, we will tear down our demo environment."
    },
    {
      "cell_type": "code",
      "id": "77906fe6-875b-4518-b699-cb20bb9fe62f",
      "metadata": {
        "language": "sql",
        "name": "sql_step08_teardown",
        "title": "sql_step08_teardown",
        "resultVariableName": "dataframe_8"
      },
      "source": "%%sql -r dataframe_8\nUSE ROLE ACCOUNTADMIN;\n\nDROP DATABASE DEMO_DB;\nDROP WAREHOUSE DEMO_WH;\nDROP ROLE DEMO_ROLE;\n\n-- Drop the weather share\nDROP DATABASE FROSTBYTE_WEATHERSOURCE;\n\n-- Remove the \"dev\" branch in your repo",
      "outputs": [],
      "execution_count": null
    }
  ]
}