{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d42b2828-6514-4731-bb9f-2117985a6ffd",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# 06 Load Excel Files\n\n* Author: Jeremiah Hansen\n* Last Updated: 2/2/2026\n\nThis notebook will load data into the `LOCATION` and `ORDER_DETAIL` tables from Excel files.\n\nThis currently does not use Snowpark File Access as it doesn't yet work in Notebooks. So for now we copy the file locally first.",
      "execution_count": null
    },
    {
      "id": "6300f36b-5d89-4f7e-95ec-7ba84941d501",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_initialize",
        "title": "py_initialize"
      },
      "source": "# Import python packages\nimport sys\nimport logging\n\n# Set up the logger\nlogger_name = 'demo_logger'\nlogger = logging.getLogger(logger_name)\nlogger.setLevel(logging.INFO)\n\n# Set default values for debugging\nnotebook_name = '06_load_excel_files.ipynb'\ndatabase_name = 'DEMO_DB'\nschema_name = 'DEV_SCHEMA'\nrole_name = 'DEMO_ROLE'\n\n# Override values with passed notebook arguments\nif sys.argv[0].endswith('.ipynb'):\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--database-name', type=str)\n    parser.add_argument('--schema-name', type=str)\n    parser.add_argument('--role-name', type=str)\n    args, args_unknown = parser.parse_known_args()\n\n    notebook_name = parser.prog  # same as argv[0]\n    database_name = args.database_name or database_name\n    schema_name = args.schema_name or schema_name\n    role_name = args.role_name or role_name\n\n# Get a Snowpark session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Set the default database and schema for the following cells\nsession.use_schema(f\"{database_name}.{schema_name}\")\n\n# Set the role (required with initial GA which uses the user's default role)\nsession.use_role(role_name)\n\n# Get details about the current state\ncurrent_state_df = session.sql(f\"\"\"\n        SELECT OBJECT_CONSTRUCT(\n            'current_user', CURRENT_USER(),\n            'current_role', CURRENT_ROLE(),\n            'current_secondary_roles', PARSE_JSON(CURRENT_SECONDARY_ROLES()),\n            'current_database', CURRENT_DATABASE(),\n            'current_schema', CURRENT_SCHEMA()\n        )::STRING AS session_context;\n    \"\"\").collect()\n\nlogger.info(f\"Begin executing notebook {notebook_name}\", extra = {'logger_name': logger_name})\nlogger.info(f\"Using parameters database: {database_name}, schema: {schema_name}, role: {role_name}\", extra = {'logger_name': logger_name})\nlogger.info(f\"Using session context {current_state_df[0]['SESSION_CONTEXT']}\", extra = {'logger_name': logger_name})",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c698b8ce-63c6-46ea-93f3-5cb40b13b46f",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_pip_install",
        "title": "py_pip_install"
      },
      "source": "!pip install openpyxl",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "387772d4-3652-403f-bf19-f47747e1224f",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_1",
        "language": "sql",
        "name": "sql_get_spreadsheets",
        "title": "sql_get_spreadsheets"
      },
      "source": "%%sql -r dataframe_1\n-- Temporary solution to load in the metadata, this should be replaced with a directy query to a directory table (or a metadata table)\nSELECT '@INTEGRATIONS.FROSTBYTE_RAW_STAGE/intro/order_detail.xlsx' AS STAGE_FILE_PATH, 'order_detail' AS WORKSHEET_NAME, 'ORDER_DETAIL' AS TARGET_TABLE\nUNION\nSELECT '@INTEGRATIONS.FROSTBYTE_RAW_STAGE/intro/location.xlsx', 'location', 'LOCATION';",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5f4fc654-1a1f-4aae-a20d-466ecd83f022",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Create a function to load Excel worksheet to table\n\nCreate a reusable function to load an Excel worksheet to a table in Snowflake.\n\nNote: Until we can use scoped URLs in Notebooks, via the `BUILD_SCOPED_FILE_URL()` function, we need to temporarily copy the file to a temp stage and then process from there."
    },
    {
      "id": "85f77673-fca8-46aa-83bd-cb0dfa8d80ba",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_load_excel_function",
        "title": "py_load_excel_function"
      },
      "source": "from snowflake.snowpark.files import SnowflakeFile\nfrom openpyxl import load_workbook\nimport pandas as pd\n\n# 1. Create a temp internal stage (once at the start)\nsession.sql(\"CREATE TEMP STAGE IF NOT EXISTS temp_excel_stage\").collect()\n\ndef load_excel_worksheet_to_table(session, external_path, worksheet_name, target_table):\n    \"\"\"Load an Excel worksheet by copying to internal stage first\"\"\"\n    \n    # Extract filename from path\n    filename = external_path.split('/')[-1]\n    \n    # 2. Copy file from external to internal stage\n    session.sql(f\"\"\"\n        COPY FILES INTO @temp_excel_stage\n        FROM {external_path}\n    \"\"\").collect()\n    \n    # 3. Now SnowflakeFile.open() works on internal stage\n    with SnowflakeFile.open(f'@temp_excel_stage/{filename}', 'rb') as f:\n        workbook = load_workbook(f)\n        sheet = workbook[worksheet_name]\n        \n    # Convert to DataFrame\n    data = sheet.values\n    columns = next(data)\n    df = pd.DataFrame(data, columns=columns)\n    \n    # Write to Snowflake table\n    snowpark_df = session.create_dataframe(df)\n    snowpark_df.write.mode(\"overwrite\").save_as_table(target_table)\n    \n    logger.info(f\"Loaded {len(df)} rows from '{worksheet_name}' to {target_table}\", extra = {'logger_name': logger_name})",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3fbc225c-332c-42e6-a5aa-e812da54e1dc",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Process all Excel worksheets\n\nLoop through each Excel worksheet to process and call our `load_excel_worksheet_to_table_local()` function."
    },
    {
      "id": "4d5b0921-3b3a-45cb-a709-476ffd962ea6",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_process_spreadsheets",
        "title": "py_process_spreadsheets"
      },
      "source": "# Process each file from the sql_get_spreadsheets cell above\nfiles_to_load = dataframe_1\nfor index, excel_file in files_to_load.iterrows():\n    print(f\"Processing Excel file {excel_file['STAGE_FILE_PATH']}\")\n    load_excel_worksheet_to_table(session, excel_file['STAGE_FILE_PATH'], excel_file['WORKSHEET_NAME'], excel_file['TARGET_TABLE'])\n\nlogger.info(f\"Finish executing notebook {notebook_name}\", extra = {'logger_name': logger_name})",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "42ddde31-f39d-442a-b788-b161c820ecdd",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Debugging"
    },
    {
      "id": "620b67d4-cde5-4599-9cf0-353023847a51",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_2",
        "language": "sql",
        "name": "sql_debugging",
        "title": "sql_debugging"
      },
      "source": "%%sql -r dataframe_2\n--DESCRIBE TABLE LOCATION;\n--SELECT * FROM LOCATION;\nSHOW TABLES;",
      "outputs": [],
      "execution_count": null
    }
  ]
}