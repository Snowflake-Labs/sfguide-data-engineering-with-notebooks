{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cad9525c-bf45-45e4-9961-26aac3ea111b",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# 07 Load Daily City Metrics\n\n* Author: Jeremiah Hansen\n* Last Updated: 1/30/2026\n\nThis notebook will load data into the `DAILY_CITY_METRICS` table with support for incremental processing.",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "61d0aa49-693f-4bc2-803e-9e351b3211c4",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_initialize",
        "title": "py_initialize"
      },
      "source": "# Import python packages\nimport sys\nimport logging\nfrom snowflake.core import Root\n\n# Set up the logger\nlogger_name = 'demo_logger'\nlogger = logging.getLogger(logger_name)\nlogger.setLevel(logging.INFO)\n\n# Set default values for debugging\nnotebook_name = '07_load_daily_city_metrics.ipynb'\ndatabase_name = 'DEMO_DB'\nschema_name = 'DEV_SCHEMA'\n\n# Override values with passed notebook arguments\nif sys.argv[0].endswith('.ipynb'):\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--database-name', type=str)\n    parser.add_argument('--schema-name', type=str)\n    args, args_unknown = parser.parse_known_args()\n\n    notebook_name = parser.prog  # same as argv[0]\n    database_name = args.database_name\n    schema_name = args.schema_name\n\n# Get a Snowpark session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Set the default database and schema for the following cells\nsession.use_schema(f\"{database_name}.{schema_name}\")\n\nlogger.info(f\"Begin executing notebook {notebook_name}\", extra = {'logger_name': logger_name})\nlogger.info(f\"Using {database_name}.{schema_name}\", extra = {'logger_name': logger_name})",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2f8be802-753e-4bcc-b80d-b7ed17c06b7d",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Create a function to check if a table exists\n\nThis function uses the [Snowflake Python Management API](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-overview)."
    },
    {
      "id": "8de74798-23fc-4de4-ab36-b158ce37faf5",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_table_exists",
        "title": "py_table_exists"
      },
      "source": "def table_exists(session, database_name='', schema_name='', table_name=''):\n    root = Root(session)\n    tables = root.databases[database_name].schemas[schema_name].tables.iter(like=table_name)\n    for table_obj in tables:\n        if table_obj.name == table_name:\n            return True\n\n    return False\n\n# Not used, SQL alternative to Python version above\ndef table_exists2(session, database_name='', schema_name='', table_name=''):\n    exists = session.sql(\"SELECT EXISTS (SELECT * FROM {}.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(database_name, schema_name, table_name)).collect()[0]['TABLE_EXISTS']\n    return exists",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6e76c9a3-7c9b-4645-bcbb-d05509d4a80b",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Pipeline to update daily_city_metrics"
    },
    {
      "id": "6c608229-284c-4a84-9627-b807e9ea8295",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_process_dcm",
        "title": "py_process_dcm"
      },
      "source": "import snowflake.snowpark.functions as F\n\ntable_name = \"DAILY_CITY_METRICS\"\n\n# Define the tables\norder_detail = session.table(\"ORDER_DETAIL\")\nhistory_day = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.HISTORY_DAY\")\nlocation = session.table(\"LOCATION\")\n\n# Join the tables\norder_detail = order_detail.join(location, order_detail['LOCATION_ID'] == location['LOCATION_ID'])\norder_detail = order_detail.join(history_day, (F.builtin(\"DATE\")(order_detail['ORDER_TS']) == history_day['DATE_VALID_STD']) & (location['ISO_COUNTRY_CODE'] == history_day['COUNTRY']) & (location['CITY'] == history_day['CITY_NAME']))\n\n# Aggregate the data\nfinal_agg = order_detail.group_by(F.col('DATE_VALID_STD'), F.col('CITY_NAME'), F.col('ISO_COUNTRY_CODE')) \\\n                        .agg( \\\n                            F.sum('PRICE').alias('DAILY_SALES_SUM'), \\\n                            F.avg('AVG_TEMPERATURE_AIR_2M_F').alias(\"AVG_TEMPERATURE_F\"), \\\n                            F.avg(\"TOT_PRECIPITATION_IN\").alias(\"AVG_PRECIPITATION_IN\"), \\\n                        ) \\\n                        .select(F.col(\"DATE_VALID_STD\").alias(\"DATE\"), F.col(\"CITY_NAME\"), F.col(\"ISO_COUNTRY_CODE\").alias(\"COUNTRY_DESC\"), \\\n                            F.builtin(\"ZEROIFNULL\")(F.col(\"DAILY_SALES_SUM\")).alias(\"DAILY_SALES\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_F\"), 2).alias(\"AVG_TEMPERATURE_FAHRENHEIT\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_IN\"), 2).alias(\"AVG_PRECIPITATION_INCHES\"), \\\n                        )\n\n# If the table doesn't exist then create it\nif not table_exists(session, database_name=database_name, schema_name=schema_name, table_name=table_name):\n    final_agg.write.mode(\"overwrite\").save_as_table(table_name)\n\n    logger.info(f\"Successfully created {table_name}\")\n# Otherwise update it\nelse:\n    cols_to_update = {c: final_agg[c] for c in final_agg.schema.names}\n\n    dcm = session.table(table_name)\n    dcm.merge(final_agg, (dcm['DATE'] == final_agg['DATE']) & (dcm['CITY_NAME'] == final_agg['CITY_NAME']) & (dcm['COUNTRY_DESC'] == final_agg['COUNTRY_DESC']), \\\n                        [F.when_matched().update(cols_to_update), F.when_not_matched().insert(cols_to_update)])\n\n    logger.info(f\"Successfully updated {table_name}\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "602e3d31-d4de-4a98-b02b-ba53b864f15d",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Debugging"
    },
    {
      "id": "1fa54381-ad5c-44be-bf63-f05e3e9ac64d",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_1",
        "language": "sql",
        "name": "sql_debugging",
        "title": "sql_debugging"
      },
      "source": "%%sql -r dataframe_1\n--SELECT * FROM DAILY_CITY_METRICS LIMIT 10;",
      "outputs": [],
      "execution_count": null
    }
  ]
}